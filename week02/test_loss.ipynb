{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.virenv/base/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import FaceDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.virenv/base/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kuba/.virenv/base/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = FaceDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((16,3,224,224))\n",
    "logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 490])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.rand((16,7,7,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import box_iou, box_convert\n",
    "\n",
    "def iou(preds, targets):\n",
    "    # Convert (x, y, width, height) to (x1, y1, x2, y2)\n",
    "    pred_x1 = preds[..., 0] - preds[..., 2] / 2\n",
    "    pred_y1 = preds[..., 1] - preds[..., 3] / 2\n",
    "    pred_x2 = preds[..., 0] + preds[..., 2] / 2\n",
    "    pred_y2 = preds[..., 1] + preds[..., 3] / 2\n",
    "        \n",
    "    target_x1 = targets[..., 0] - targets[..., 2] / 2\n",
    "    target_y1 = targets[..., 1] - targets[..., 3] / 2\n",
    "    target_x2 = targets[..., 0] + targets[..., 2] / 2\n",
    "    target_y2 = targets[..., 1] + targets[..., 3] / 2\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    x1 = torch.max(pred_x1, target_x1)\n",
    "    y1 = torch.max(pred_y1, target_y1)\n",
    "    x2 = torch.min(pred_x2, target_x2)\n",
    "    y2 = torch.min(pred_y2, target_y2)\n",
    "    \n",
    "    intersection = torch.clamp((x2 - x1), min=0) * torch.clamp((y2 - y1), min=0)\n",
    "    \n",
    "    # Calculate union area\n",
    "    pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
    "    target_area = (target_x2 - target_x1) * (target_y2 - target_y1)\n",
    "    union = pred_area + target_area - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection / (union + 1e-7)\n",
    "    return iou\n",
    "\n",
    "# This is YOLO loss but we have no class probs just one class\n",
    "class Yolo_Loss(nn.Module):\n",
    "    def __init__(self, S=7, B=2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduce=\"sum\")\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "\n",
    "        self.lambda_coord = 5\n",
    "        self.lambda_no_obj = 0.5\n",
    "\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        prediction = prediction.view(-1, self.S, self.S, self.B*5)\n",
    "        iou0 = iou(prediction[..., 1:5], target[..., 1:5]) #prediction is (S*S, B*5) => (S*S, B*(confidence, x,y,wh))\n",
    "        iou1 = iou(prediction[..., 6:10], target[..., 6:10]) #prediction is (S*S, B*5) => (S*S, B*(confidence, x,y,wh))\n",
    "        ious = torch.cat([iou0.unsqueeze(0), iou1.unsqueeze(0)], dim=0)\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        bestbox = bestbox.unsqueeze(dim=-1)\n",
    "\n",
    "        exits_box = target[..., 0].unsqueeze(3)\n",
    "\n",
    "        # For box coord\n",
    "        #best box tells use which box has teh higher iou then we use that as teh prediction\n",
    "        box_preds = exits_box * ((bestbox * prediction[..., 6:10]) + ((1- bestbox) * prediction[..., 1:5]))\n",
    "        # Take the square root of width and height to handle size boxe differences\n",
    "        box_preds[..., 2:4] = torch.sign(box_preds[..., 2:4]) * torch.sqrt(torch.abs(box_preds[..., 2:4] + 1e-7))\n",
    "    \n",
    "\n",
    "        #since we are prediction only one class the target vector will be (S,S,5)\n",
    "        box_targets = exits_box * target[..., 1:5]\n",
    "        #sqrt of the target witdh and height too\n",
    "        box_targets[..., 2:4] = torch.sign(box_targets[..., 2:4]) * torch.sqrt(torch.abs(box_targets[..., 2:4]) + 1e-7)\n",
    "        \n",
    "        # we flatten so that is (N*S*S, 4) and we jsut coment the values\n",
    "        box_loss = self.mse(box_preds.flatten(end_dim=-2), box_targets.flatten(end_dim=-2))\n",
    "\n",
    "        #for the next part of teh loss we need to do confidence of object fior the ones that do have a true object\n",
    "        pred_confidence = (bestbox * prediction[..., 5:6]) + ((1- bestbox) * prediction[..., 0:1])\n",
    "        target_confidence = target[..., 0:1]\n",
    "\n",
    "        # OBJECT LOSS\n",
    "        # This is where there is a box and model shouwl have predicated it\n",
    "        obj_loss = self.mse((exits_box * pred_confidence).flatten(end_dim=-2), (exits_box * target_confidence).flatten(end_dim=-2))\n",
    "        # obj_loss = self.mse(exits_box * pred_confidence, (1 - exits_box) * target_confidence)\n",
    "\n",
    "\n",
    "        # NO OBJECT LOSS\n",
    "        # This is for when the model predicts an obj but their is non\n",
    "        no_obj00_loss = self.mse(((1 - exits_box) * prediction[..., 0:1]).flatten(end_dim=-2), ((1 - exits_box) * target_confidence).flatten(end_dim=-2))\n",
    "        no_obj01_loss = self.mse(((1 - exits_box) * prediction[..., 5:6]).flatten(end_dim=-2), ((1 - exits_box) * target_confidence).flatten(end_dim=-2))\n",
    "\n",
    "        loss = (self.lambda_coord * box_loss + obj_loss + self.lambda_no_obj * (no_obj00_loss + no_obj01_loss))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.virenv/base/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = Yolo_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(logits, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1094, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got -2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got -2)"
     ]
    }
   ],
   "source": [
    "loss.flatten(end_dim=-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss[0][0][0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss.unsqueeze(dim=-1) * ped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[..., 0].unsqueeze(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newshape_preds = logits.view(-1, 7, 7, 2*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newshape_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newshape_preds[..., 1:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newshape_preds[..., :10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[...,1:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newshape_preds = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou(newshape_preds[..., 1:5], label[...,1:5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor((0,1,2,3,4,5,6,7,8,9))[0:1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
