
optimizer type: Adagrad lr: 0.0003 
Other paramiters: ['lr_decay: 0', 'eps: 1e-10', 'weight_decay: 0', 'initial_accumulator_value: 0', 'foreach: None', 'maximize: False', 'differentiable: False']   

EPOCHS: 250

Batchsize: train:32 dev:128

Dropout: 0.2
