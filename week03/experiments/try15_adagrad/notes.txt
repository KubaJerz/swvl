
optimizer type: AdamW lr: 3e-05 
Other paramiters: ['betas: (0.9, 0.999)', 'eps: 1e-08', 'weight_decay: 0.001', 'amsgrad: True', 'foreach: None', 'maximize: False', 'capturable: False', 'differentiable: False', 'fused: None', 'initial_lr: 3e-05']   

EPOCHS: 250

Batchsize: train:32 dev:128

Dropout: 0.5

scheduler type: MultiStepLR 
paramiters: ['milestones:Counter({60: 1})', 'gamma:2', 'base_lrs:[3e-05]', 'last_epoch:0', 'verbose:False', '_get_lr_called_within_step:False', '_last_lr:[3e-05]']
